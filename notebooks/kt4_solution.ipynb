{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, year, to_date\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import requests\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/jupyterhub/miniconda/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/jupyterhub/miniconda/bin/python'\n",
    "\n",
    "# Создаем SparkSession с поддержкой HDFS\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Data to HDFS\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master:9000\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Проверка подключения\n",
    "print(\"SparkSession успешно создан!\")\n",
    "print(\"Версия Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load(target_year, url, max_rows=10000):\n",
    "    print(f\"Скачиваем данные за {target_year} год...\")\n",
    "    \n",
    "    # Скачиваем данные\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Читаем только нужное количество строк\n",
    "    lines = []\n",
    "    line_count = 0\n",
    "    for line in response.iter_lines():\n",
    "        if max_rows is None or line_count <= max_rows:\n",
    "            lines.append(line.decode('utf-8'))\n",
    "            line_count += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Создаем pandas DataFrame из строк\n",
    "    pdf = pd.read_csv(StringIO('\\n'.join(lines)))\n",
    "    \n",
    "    pdf[\"tpep_pickup_datetime\"] = pdf[\"tpep_pickup_datetime\"].astype(str)\n",
    "    pdf[\"tpep_dropoff_datetime\"] = pdf[\"tpep_dropoff_datetime\"].astype(str)\n",
    "    pdf[\"store_and_fwd_flag\"] = pdf[\"store_and_fwd_flag\"].astype(str)\n",
    "    \n",
    "    # Конвертируем в Spark DataFrame\n",
    "    df = spark.createDataFrame(pdf)\n",
    "    \n",
    "    # Парсим дату с учетом AM/PM\n",
    "    df = df.withColumn(\n",
    "        \"tpep_pickup_datetime\", \n",
    "        to_timestamp(col(\"tpep_pickup_datetime\"), \"MM/dd/yyyy h:mm:ss a\")\n",
    "    ).withColumn(\n",
    "        \"tpep_dropoff_datetime\", \n",
    "        to_timestamp(col(\"tpep_dropoff_datetime\"), \"MM/dd/yyyy h:mm:ss a\")\n",
    "    )\n",
    "    \n",
    "    # Фильтруем по году\n",
    "    result_df = df.filter(year(col(\"tpep_pickup_datetime\")) == target_year)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Загружаем данные\n",
    "taxi_2019 = download_and_load(2019, \"https://data.cityofnewyork.us/api/views/2upf-qytp/rows.csv?accessType=DOWNLOAD\")\n",
    "taxi_2020 = download_and_load(2020, \"https://data.cityofnewyork.us/api/views/kxp8-n2sj/rows.csv?accessType=DOWNLOAD\")\n",
    "taxi_2021 = download_and_load(2021, \"https://data.cityofnewyork.us/api/views/m6nq-qud6/rows.csv?accessType=DOWNLOAD\")\n",
    "\n",
    "# Проверяем количество строк\n",
    "print(f\"2019: {taxi_2019.count()} записей\")\n",
    "print(f\"2020: {taxi_2020.count()} записей\")\n",
    "print(f\"2021: {taxi_2021.count()} записей\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9605c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем схему для приведения типов\n",
    "taxi_schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"RatecodeID\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Функция для очистки данных\n",
    "def clean_taxi_data(df):\n",
    "    # Приводим типы данных\n",
    "    df = spark.createDataFrame(df.rdd, taxi_schema)\n",
    "    \n",
    "    # Обрабатываем пропущенные значения\n",
    "    df = df.fillna({\n",
    "        \"passenger_count\": 1,\n",
    "        \"trip_distance\": 0.0,\n",
    "        \"RatecodeID\": 1,\n",
    "        \"payment_type\": 1,\n",
    "        \"fare_amount\": 0.0,\n",
    "        \"extra\": 0.0,\n",
    "        \"mta_tax\": 0.0,\n",
    "        \"tip_amount\": 0.0,\n",
    "        \"tolls_amount\": 0.0,\n",
    "        \"improvement_surcharge\": 0.0,\n",
    "        \"total_amount\": 0.0,\n",
    "        \"congestion_surcharge\": 0.0\n",
    "    })\n",
    "    \n",
    "    # Добавляем столбец с датой для партиционирования\n",
    "    df = df.withColumn(\"pickup_date\", to_date(col(\"tpep_pickup_datetime\")))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Очищаем данные для каждого года\n",
    "taxi_2019_clean = clean_taxi_data(taxi_2019)\n",
    "taxi_2020_clean = clean_taxi_data(taxi_2020)\n",
    "taxi_2021_clean = clean_taxi_data(taxi_2021)\n",
    "\n",
    "# Проверяем схему после очистки\n",
    "taxi_2020_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6444383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сохранения данных\n",
    "def save_to_hdfs(df, year):\n",
    "    output_path = f\"hdfs://master:9000/user/alice/data/yellow_taxi/{year}\"\n",
    "    df.write \\\n",
    "        .partitionBy(\"pickup_date\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_path)\n",
    "    print(f\"Данные за {year} год сохранены в {output_path}\")\n",
    "\n",
    "# Сохраняем данные за каждый год\n",
    "save_to_hdfs(taxi_2019_clean, \"2019\")\n",
    "save_to_hdfs(taxi_2020_clean, \"2020\")\n",
    "save_to_hdfs(taxi_2021_clean, \"2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для проверки сохраненных данных\n",
    "def check_hdfs_data(year):\n",
    "    path = f\"hdfs://master:9000/user/alice/data/yellow_taxi/{year}\"\n",
    "    df = spark.read.parquet(path)\n",
    "    print(f\"Проверка данных за {year} год:\")\n",
    "    print(f\"Количество записей: {df.count()}\")\n",
    "    print(f\"Даты партиций: {df.select('pickup_date').distinct().count()} уникальных дней\")\n",
    "    df.show(5)\n",
    "\n",
    "check_hdfs_data(\"2019\")\n",
    "check_hdfs_data(\"2020\")\n",
    "check_hdfs_data(\"2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
